Lesson 3: Advanced Lambda and Stream Concepts
=============================================

3-1: Understand and Using Reductions
------------------------------------

Simple problem: find the length of the longest line in a file

	Path input = Paths.get("lines.txt");
	int longestLineLength = Files.lines(input)
		.mapToInt(String::length)
		.max()
		.getAsInt();

Another simple problem: find the longest line in a file

NaÃ¯ve stream solution

	String longest = Files.lines(input)
		.sort((x, y) -> y.length() - x.length())
		.findFirst()
		.get();

* Big files will take a long time and a lot of resources.
* Must be a better approach

External Iteration Solution

	String longest = "";
	while ((String s = reader.readLine()) != null)
		if (s.length() > longest.length())
			longest = s;

* Simple, but inherently serial.
* Not thread safe due to mutable state.
* Not functional.

Recursive approach: the method

	String findLongestString(String s, int index, List<String> l)
	{
		if (index >= l.size())
			return s;

		if (index == l.size() - 1) {
			if (s.length() > l.get(index).length())
				return s;
			return l.get(index);
		}

		String s2 = findLongestString(l.get(index), index + 1, l);

		if (s.length() > s2.length())
			return s
		return s2;
	}

Recursive approach: solving the problem

	List<String> lines = new ArrayList<>();
	while ((String s = reader.readLine()) != null)
		lines.add(s);
	String longest = findLongestString("", 0, lines);

* No explicit loop, no ;utable state, so functional solution.
* Not a usable one: larger data sets will generate an OOM exception.

A Better Stream Solution

* The stream API uses the well known filter-map-reduce pattern.
* For this problem we don't need to filter() or map(), just reduce().
* Recall the recude method definition:
	Optional<T> reduce(BinaryOperator<T> accumulator)
* The key is to find the right accumulator
	* Again, recall the accumulator takes a partial result and the next element, and returns a new partial result.
	* In essence it does the same as our recursive solution.
	* Without all the stack frames.
* Use the recursive approach as an accumulator for a reduction:
	
	String longestLine = Files.lines(input)
		.reduce((x, y) _> {
			if (x.length() > y.length())
				return x; // x in effect maintains the state for us, by always holding the longtest string found so far.
			return y;
		})
		.get();

The Simplest Stream Solution

* Use a specialised form of max()
* One that takes a Comparator as a parameter
	Files.lines(input)
		.max(comparingInt(String::length))
		.get();
* comparingInt() is a static method on Comparator
	Comparator<T> comparingInt(ToIntFunction<? extends T> keyExtractor)

Summary

* Reduction take a stream and reduces it to a single value
* The way reduction works is defined by the accumulator
	* Which is a BinaryOperator
	* The accumulator is applied successively to the stream elements
	* The reduce() method maintains a partial result state
	* Like a recursive approach, but without the resource overhead
* Requires you to think differently to an imperative, loop based approach.

3-2: Finite and Infinite Streams
--------------------------------

Dealing with the inderterminate in Imperative Java

* How to continue processing when we can't predict for how long?

	while(true) {
		doSomeProcessing();
		if (someCriteriaIsTrue())
			break;
		// Loop repeats indefinitely
	}

Using Infinite Streams: Making the Stream finite

* Terminate the stream when a condition is met:

	findFirst(Predicate p) // deterministic results whether we use a sequential or parallel stream
	findAny(Predicate p) // non deterministic results

	int r = Random.ints() // infinite stream of random integers
		.findFirst(i -> i > 256); // stream terminates when a number greater than 256 is encountered

Using Infinite Streams: Keeping it Infinite

* Sometimes we need to continue to use a stream indefinitely
* What terminal operation should we use for this?
	* Use forEach()
	* This consumes each element from the stream
	* But does not terminate it

Infinite example: reading temperature from a serial sensor
	* Converting from farenheit to celcius, removing F
	* Notifying a listener of changes if registered

	thermalReader.lines()
		.mapToDouble(s -> Double.parseDouble(s.substring(0, s.length() - 1)))
		.map(t -> ((t - 32) * 5 / 9)) // conversion
		.filter(t -> currentTemperature.equals(t)) // test whether the current temperature is different from the new temperature
		.peek(t -> listener.ifPresent(l -> l.temperatureChanged(t))) // take any values that are different and update the listener if present
		.forEach(t -> currentTemperature.set(t)); // consume the temperature and set the current temperature with the new value

Summary

* Streams can be infinite as well as finite
* There is no concept of "breaking" out of a stream, we can terminate the stream using findFirst of findAny
* Use the appropriate terminal operator to stop processing
* Or use the infinite stream infinitely.


3-3: Avoiding the use of forEach
---------------------------------

Using streams effectively: stop thinking imperatively

* Imperative programming uses loops for repetitive behaviour
* It also uses variables to hold state
* We can continue to do that in some ways with streams but
* THIS IS WRONG

Stream example: Still thinking imperatively

	List<Transactions> transactions = ...
	LongAdder transactionTotal = new LongAdder();
	transactions.stream()
		.forEach(t -> transactionTotal.add(t.getValue())); // we are modifying state which is wrong for a functional approchach
	long total = transactionTotal.sum();

Stream example: Correct functional approach

	List<Transactions> transactions = ...
	long total = transactions.stream()
		.mapToLong(t -> t.getValue()) // create a stream of long values that is passed to the next function
		.sum(); // use a reduction to create a single result

Legitimate use of forEach: No state being modified

* Simplfied iteration
* May be made parallel if order is not important
	
	transactions.stream()
		.forEach(t -> t.printClientName());

Summary

* If you are thinking of using forEach(), stop
* Can it be replaced with a combination of mapping and reduction?
* IF so, it is unlikely to be the right approach to be functional
* Certain situations are valid for using forEach()
	e.g. printing values from the stream (consuming the values from the stream)

3-4: Using Collectors
---------------------

Collector Basics

* A Collector performs a mutable reduction on a stream
	* Accumulates input elements into a mutable result container
	* Results container can be a List, Map, String, etc
* Use the collect() method to terminate the stream.
* Collectors utility class has many methods that can create a Collector

Composing Collectors

* Several Collectors methods have versions with a downstream collector
* Allows a second collector to be used:
	collectingAndThen()
	groupingBy() / groupingByConcurrent()
	mapping()
	partitioningBy()

Collecting into a Collection

* toCollection(Supplier factory)
	adds the leements of the stream to a Collection (created using factory)
	uses encounter order
* toList()
	adds the elements of the stream to a List
* toSet()
	adds the elements of the stream to a Set
	eliminates duplicates

Collecting to a Map

* toMap(Function keyMapper, Function valueMapper)
	creates a Map from the elements of the stream
	key and value produced using provided functions
	use Function.identity() to get the stream element

		Map<Student, Double> studentToScore = sutends.stream()
			.collect(toMap(Functions.identity(), // use the student as key
							student -> getScore(Student)));

Collecting to a Map: Handling duplicate keys

* toMap(Function keyMapper, Function valueMapper, BinaryOperator merge)
	Map<String, String> occupants = people.stream()
		.collect(toMap(Person::getAddress,
						Person::getName,
						(x, y) -> x + ", " + y)); // people living at the same address are concatenated

Grouping results

* groupingBy(Function)
	groups stream elements using the Function into a Map
	result is Map<K, List<V>>
		Map m = words.stream()
			.collect(Collectors.groupingBy(String::length)); // length is key, list of words of this length is value
* groupingBy(Function, Collector)
	a reduction is performed on each group using the downstream Collector
		Map m = words.stream()
			.collect(Collectors.groupingBy(String::length, counting())) // length is key, value is number of words of this length

Joining String results

* joining()
	Collector concatenates input strings
* joining(delimiter)
	concatenates stream string using CharSequence delimiter
		collect(Collectors.joining(",")); // create csv
* joining(delimiter, prefix, suffix)

Numeric collectors (also available in double and long forms)

* averingInt(ToIntFunction)
	averages the results generated by the supplied function
* summarizingInt(ToIntFunction)
	summarises (count, sum, min, max, average) result generated by the supplier function
* summingInt(ToIntFunction)
	equivalent to a map() then sum()
* maxBy(Comparator), minBy(Comparator)
	max or min value based on Comparator

Other Collectors

* reducing(BinaryOperator)
	equivalent Collector to reduce() terminal operation
	only use for multi-level reductions, or downstream collectors
* partitioningBy(Predicate)
	create a Map<Boolean, List> containing two groups based on Predicate
* mapping(Function, Collector)
	adapts a Collector to accept different type elements mapped by the Function
		Map<City, Set<String>> lastNamesByCity = people.stream()
			.collect(groupingBy(Person::getCity,
								mapping(Person::getLastName, toSet())));

Summary

Collectors provide powerful ways to gather elements of an input stream
	into collections
	in numerical ways like totals and averages
Collectors can be composed to build more complex ones
You can also create your own Collector

3-5: Parallel Streams (And When Not to Use Them)
------------------------------------------------

Serial and Parallel Streams

* Collection stream sources
	stream()
	parallelStream()
* Stream can be made parallel or sequential at any point
	parallel()
	sequential()
* The last call wins
	whole stream is either sequential or parallel
* Calling concat() with a sequential and parallel stream will produce a parallel stream.

Parallel Streams

* Implemented internally using the fork-join framework
* Will default to as many threads for the pool as the OS reports processors
	which may not be what you want
		System.setProperty(
			"java.util.concurrent.ForkJoinPool.common.parallelism",
			"32767"); // 0 to 32767
* Remember, parallel streams always need more work to process
	but they might finish it more quickly

Parallel Stream Considerations

* findFirst() and findAny()
	findAny() is non-deterministic, so better for parallel stream performance
	use findFirst() if a deterministic result is required (even in a parallel stream)
* forEach() and forEachOrdered()
	forEach() is non-deterministic for a parallel stream and ordered data
	use forEachOrdered() is a deterministic result is required

When to use parallel Stream?

* No simple anwser
* Data set size is important, as is the type of data structure
	ArrayList: GOOD
	HashSet, TreeSet: OK
	LinkedList: BAD
* Operations are also important
	certain operations decompose to parallel tasks better than others
	filter() and map() are excellent (operation on each and every element, no reliance on other elements)
	sorted() and distinct() do not decompose well (reliance on other elements)
* Quantitative considerations
	N = size of the data set
	Q = cost per element through the stream pipeline
	N x Q = total cost of pipeline operations
	The bigger N x Q is the better a parallel stream will perform
	It is easier to know N than Q, but Q can be estimated
	If in doubt, profile

Summary

Streams can be processed sequentially or in parallel
	the whole stream is processed sequentially or in parallel
	in most cases how the stream is defined wil not affect the result
	findFirst(), findAny(), forEach(), forEachOrdered() do
Don't assume that a parallel stream will return a result faster
	many factors affect performance

3-6: Debugging Lambdas and Streams
----------------------------------

Problems with debugging streams

* Stream provide a high level abstraction
	this is good for making code clear and easy to understand
	this is bad or debugging
		a lot happens internally in the library code
		setting breakpoints is not simple
		stream operations are merged to improve efficiency

Simple debugging: finding what is happening between methods

* Use peek()
	like the use of print statements
		List<String> sortedWords = reader.lines() 			// lines from file
			.peek(System.out::println)						// print lines from file
			.flatMap(line -> Stream.of(line.split(REGEXP))) // words from file
			.peek(System.out::println)						// print words
			.map(String::toLowerCase) 						// in lower case
			.distinct() 									// remove duplicates
			.sort((x, y) -> x.length() - y.length()) 		// sort by length
			.collect(Collectors.toList())); 				// collect to list

Setting a breakpoint: Using peek()

* add a peek() method call between stream operations
* Use a Consumer that does nothing if required
	some debugging tools don't like empty bodies

		List<String> sortedWords = reader.lines() 			// lines from file
			.peek(s -> s)									// no-op lambda, set breakpoint here
			.flatMap(line -> Stream.of(line.split(REGEXP))) // words from file
			.map(String::toLowerCase) 						// in lower case
			.distinct() 									// remove duplicates
			.sort((x, y) -> x.length() - y.length()) 		// sort by length
			.collect(Collectors.toList())); 				// collect to list

Setting a breakpoint using a method reference

* Lambda expressions do not compile to equivalent inner class
	compiled to invokedynamic call
	implementation decided at runtime
	better chance of optimisation, makes debugging harder
* Solution
	extract the code from a Lambda expression into a separate method
	replace the Lambda with a method reference for the new method
	set breakpoints on the statements in the new method
	examine programe state using debugger

Summary

Debugging is harder with Lambdas and streams
	Stream methods get ;erged
	Lambdas are concerted to invokedynamic bytecodes and implementation is decided at runtime
	harder to set breakpoints
peek() and method references can simplify things

3-7: Course Conclusion
----------------------

Lambda expressions

* Give us a simple way to define behaviour
	can be assigned to a variable or passed as a parameter
* Can be used wherever the type is a functional interface
	one that has only one abstract method
	the lambda expression provides an implementation of the abstract method

Stream API

* Pipeline of operations to process collections of data
	multiple sources, not just from the Collections API
	can be processed sequentially or in parallel
* Sources, intermediate and terminal operations
* Behaviour of intermediate and terminal operations often defined using Lambda expressions
* Terminal operations ofter return an Optional
* We can now used a functional style of programming in Java

Lambdas and Streams: Think Differently

* Need to think functional rather than imperative
	try to stop thinking in loops and using mutable state
* Think of how to approach problems using recursion
	rather than an explicit loop
	avoid forEach (except for special cases)
* Infinite streams don't need to be infinite
* Remember, parallel streams always involve more work
	sometimes they complete the work quicker

